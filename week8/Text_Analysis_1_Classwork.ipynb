{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Εβδομάδα 1 - Εξόρυξη και προετοιμασία κειμένων \n",
    "\n",
    "\n",
    "**Θεωρία**\n",
    "\n",
    "Η εξόρυξη κειμένων γίνεται όλο και πιο σημαντική αυτά τα χρόνια καθώς έχουν αυξηθεί οι εφαρμογές ιστού που δημιουργούν τέτοια δεδομένα σε μεγάλες ποσότητες. Η εξόρυξη γνώσης γενικά περιγράφεται ως η εξερεύνηση και ανάλυση με αυτόματες ή ημιαυτόματες μεθόδους μεγάλου όγκου δεδομένων για την ανακάλυψη προτύπων και κανόνων με νόημα και εφαρμόζεται στην αναγνώριση σχέσεων σε μεγάλα πολυδιάστατα σύνολα δεδομένων. Ενώ κλασσικές εφαρμογές έχουν συγκεντρωθεί στην επεξεργασία και εξόρυξη κειμένου, η εμφάνιση αυτών των εφαρμογών ιστού απαιτεί καινούριες μεθόδους εξόρυξης και επεξεργασίας, όπως τη συσχέτιση αυτών, στη πολυγλωσσική πληροφορία και την εξόρυξη κειμένων - πολυμέσων όπως εικόνες ή βίντεο. Το κύριο χαρακτηριστικό που κάνει τα δεδομένα κειμένου να διαφέρουν από άλλες μορφές δεδομένων είναι η αραιότητα τους και η υψηλή τους διαστατικότητα. Για παράδειγμα μια συλλογή κειμένων μπορεί να αποτελείται από ένα λεξικό από χιλιάδες λέξεις (π.χ. το αγγλικό λεξικό Oxford Dictionary) αλλά ένα κείμενο μπορεί να περιέχει λίγες εκατοντάδες λέξεις.\n",
    "\n",
    "**Εφαρμογές και μέθοδοι εξόρυξης κειμένου**\n",
    "\n",
    "Η εξόρυξη κειμένου ή ανακάλυψη γνώσης από κείμενα αναφέρεται στη διαδικασία της εξαγωγής πληροφορίας υψηλής ποιότητας από κείμενα (π.χ. δομημένα όπως σε μορφή csv, ημιδομημένα όπως XML,JSON ή αδόμητα όπως κείμενα, βίντεο και εικόνες). Αυτή καλύπτει μια μεγάλη γκάμα θεματολογίας και αλγορίθμων για την ανάλυση κειμένων που επεκτείνεται σε πολλές κοινότητες όπως την ανάκτηση πληροφορίας, επεξεργασία φυσικής γλώσσας και μηχανική μάθηση.\n",
    "\n",
    "**Information Retrieval (IR)** : Η ανάκτηση πληροφορίας είναι η διαδικασία εύρεσης πηγών πληροφορίας (συνήθως κείμενα) από μια συλλογή αδόμητων συνόλων δεδομένων. Επομένως η ανάκτηση πληροφορίας επικεντρώνεται περισσότερο στην διευκόλυνση εύρεσης της πληροφορίας παρά την ανάλυση της και την εύρεση προτύπων σε αυτή. Η ανάκτηση πληροφορίας δίνει λιγότερη προτεραιότητα στην επεξεργασία ή στο μετασχηματισμό του κειμένου ενώ η εξόρυξη γνώσης πάει ένα βήμα παρακάτω και αναλύει την πληροφορία για καλύτερη κατανόηση αυτής.\n",
    "\n",
    "\n",
    "Ξεκινάμε με εισαγωγή κειμένων διαφόρων μορφών (PDFs, HTML, Word) και κατάλληλη προετοιμασία, ώστε να μπορούν να \"διαβαστούν\" και να αναλυθούν από τους υπολογιστές. \n",
    "\n",
    "Στο συγκεκριμένο notebook θα εξετάσουμε με ποιον τρόπο μπορούμε να ανακτήσουμε κέιμενα που βρίσκονται αναρτημένα στο διαδίκτυο (scraping), σε μορφή HTML, PDF και Word. Στην συνέχεια θα δοκιμάσουμε να περιηγηθούμε μέσω υπερσυνδέσεων για να δημιουργήσουμε ένα δείγμα από online περιεχόμενο, με την τεχνική \"spidering\" ή με την χρήση APIs, (Application Programming Interfaces), που παρέχονται ορισμένες φορές από τις διάφορες εταιρείες σε προγραμματιστές και παρέχουν πρόσβαση στο περιεχόμενό τους. \n",
    "\n",
    "Στην συνέχεια των μαθημάτων θα δοκιμάσουμε regular expressions, για να \"καθαρίσουμε\" τα κείμενα από ανεπιθύμητο περιεχόμενο όπως σημεία στίξης, μορφοποιήσεις κ.λπ.\n",
    "\n",
    "Στο τέλος θα εξερευνήσουμε τους τρόπους με τους οποίους μπορούμε να αποθηκέυσουμε το κείμενο, ώστε να είναι έτοιμο για ανάλυση. \n",
    "\n",
    "Για το συγκεκριμένο notebook θα χρειαστούμε τα εξής πακέτα: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lucem_illud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d2e9466c19bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Η συγκεκριμένη βιβλιοθήκη παρέχει πρόσβαση σε διάφορες βιβλιοθήκες που θα χρειαστούμε.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlucem_illud\u001b[0m \u001b[0;31m#pip install git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lucem_illud'"
     ]
    }
   ],
   "source": [
    "#Η συγκεκριμένη βιβλιοθήκη παρέχει πρόσβαση σε διάφορες βιβλιοθήκες που θα χρειαστούμε.\n",
    "import lucem_illud #pip install git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Τα παρακάτω πακέτα θα εγξατασταθούν με την εντολή pip\n",
    "import requests #βιβλιοθήκη για http requests\n",
    "import bs4 #ονομάζεται `beautifulsoup4`, και είναι ένας html parser\n",
    "import pandas #δημιουργεί DataFrames\n",
    "import docx #διαβάζει MS doc files, για εγκατάσταση θέλουμε το `python-docx`\n",
    "\n",
    "#Πακέτο για επεξεργασία pdfs\n",
    "#Εγξατάσταση ως `pdfminer2`\n",
    "import pdfminer.pdfinterp\n",
    "import pdfminer.converter\n",
    "import pdfminer.layout\n",
    "import pdfminer.pdfpage\n",
    "\n",
    "#Tα παρακάτω υπάρχουν στην Python\n",
    "import re #regexs\n",
    "import urllib.parse #ένωση urls\n",
    "import io #κάνει τα http requests να μοιάζουν με αρχεία\n",
    "import json #API responses\n",
    "import os.path #Τσεκάρει ότι υπάρχουν τα αρχεία \n",
    "import os #Δημιουργεί directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα δουλέψουμε επίσης με τα παρακάτω αρχεία και urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "example_text_file = 'sometextfile.txt'\n",
    "information_extraction_pdf = 'https://github.com/Computational-Content-Analysis-2018/Data-Files/raw/master/1-intro/Content%20Analysis%2018.pdf'\n",
    "example_docx = 'https://github.com/Computational-Content-Analysis-2018/Data-Files/raw/master/1-intro/macs6000_connecting_to_midway.docx'\n",
    "example_docx_save = 'example.docx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "Πριν ξεκινήσει η ανάλυση περιεχομένου πρέπει να έχουμε το περιεχόμενο στην κατοχή μας. Μερικές φορές μπορεί να φτάσει στα χέρια μας ήδη \"καθαρό\" και έτοιμο για χρήση ως αρχείο κειμένου και άλλες φορές θα χρειαστεί να το κατεβάσουμε από το διαδίκτυο.\n",
    "\n",
    "Για αρχή ας δοκιμάσουμε να κατεβάσουμε μια σελίδα από το wikipedia για ανάλυση περιεχομένου. \n",
    "\n",
    "Η σελίδα βρίσκεται στο (https://en.wikipedia.org/wiki/Content_analysis)\n",
    "\n",
    "Θα χρησιμοποιήσουμε μία HTTP GET request για το συγκεκριμένο url, μια GET request είναι ένα απλό αίτημα στον server για να μας δώσει το περιεχόμενο αυτού του url. Ένα άλλο είδος είναι το POST request και πρόκειται για ένα αίτημα στο οποίο ζητάμε εμείς από τον server να πάρει ένα αρχείο που του δίνουμε. \n",
    "\n",
    "Παρόλο που η Python έχει ήδη μια βιβλιοθήκη για GET requests, εμείς θα χρησιμοποιούμε την \n",
    "[_requests_](http://docs.python-requests.org/en/master/) γιατί είναι πιο εύχρηστη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "requests.get(wikipedia_content_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'Response [200]'` σημαίνει ότι ο server ανταποκρίθηκε σε αυτό που του ζητήσαμε. Αν παίρναμε άλλον αριθμό (π.χ. 404) θα σήμαινε ότι υπάρχει κάποιο error. \n",
    "\n",
    "Υπάρχει λίστα με όλους τους διαθέσιμους κωδικούς από HTTP response codes εδώ: \n",
    "[here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). \n",
    "\n",
    "To response διαθέτει όλα τα data που μας έστειλε ο server, το περιεχόμενο του website και το HTTP header. Εμάς μας ενδιαφέρει το περιεχόμενο που μπορούμε να το προσπελάσουμε με την εντολή `.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiContentRequest = requests.get(wikipedia_content_analysis)\n",
    "print(wikiContentRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό δεν είναι το αποτέλεσμα που περιμέναμε, καθώς πρόκειται για την αρχή της  HTML του website. Η γλώσσα αυτή διαβάζεται από υπολογιστές, για το λόγο αυτό θα χρησιμοποιήσουμε έναν parser για να τη διαβάσουμε. Για το parsing θα χρησιμοποιήσουμε το [_Beautiful\n",
    "Soup_](https://www.crummy.com/software/BeautifulSoup/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiContentSoup = bs4.BeautifulSoup(wikiContentRequest.text, 'html.parser')\n",
    "print(wikiContentSoup.text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό είναι καλύτερο από το προηγούμενο αποτέλεσμα, αλλά και πάλι περιέχει πολά κενά και όχι μόνο κείμενο. Αυτό συνέβη γιατί ζητήσαμε όλη την webpage, και όχι μόνο το κείμενο.\n",
    "\n",
    "Εμείς θέλουμε μόνο το κείμενο και για να πετύχουμε να πάρουμε μόνο αυτό, πρέπει να ελέγξουμε την html. Ένας εύκολος τρόπος να το κάνουμε αυτό είναι να επισκεφθούμε το website με έναν browser και να χρησιμοποιήσουμε το inspection ή το \"view source tool\". Σε περίπτωση που υπάρχει javascript ή κάποιο άλλο είδος δυναμικού loading στην σελίδα, τότε πρέπει να ελέγξουμε αυτό που λαμβάνει η Python και χρειαζόμαστε τη βιβλιοθήκη `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "\n",
    "with open(content_analysis_save, mode='w', encoding='utf-8') as f:\n",
    "    f.write(wikiContentRequest.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα ας ανοίξουμε το αρχείο (`wikipedia_content_analysis.html`) που μόλις δημιουργήσαμε με τον web browser. Πρέπει να μοιάζει όπως το original αλλά χωρίς τις εικόνες και τη μορφοποίηση. \n",
    "\n",
    "Επειδή δεν υπάρχει τίποτα θέσφατο στη δημιουργία μιας ιστοσελίδας, το να καταλάβουμε τι πραγματικά είναι χρήσιμο για μας είναι Τέχνη. Κοιτώντας σε αυτή την σελίδα φαίνεται πως το κυρίως κείμενο είναι μέσα στα `<p>`(paragraph) tags που βρίσκονται μέσα στο `<body>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentPTags = wikiContentSoup.body.findAll('p')\n",
    "for pTag in contentPTags[:3]:\n",
    "    print(pTag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα έχουμε όλο το κείμενο χωρισμένο σε παραγράφους.  \n",
    "\n",
    "Μας μένει να κάνουμε κάτι ακόμα πριν αρχίσουμε την επεξεργασία, να βγάλουμε τα references indicators (`[2]`, `[3]` , κ.λπ). Για να γίνει αυτό χρειάζεται λίγο regular expression (regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentParagraphs = []\n",
    "for pTag in contentPTags:\n",
    "    contentParagraphs.append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "\n",
    "#μετατροπή σε DataFrame\n",
    "contentParagraphsDF = pandas.DataFrame({'paragraph-text' : contentParagraphs})\n",
    "print(contentParagraphsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα έχουμε ένα `DataFrame` που περιέχει όλα τα σχετικά κείμενα από την ιστοσελίδα. \n",
    "\n",
    "Επειδή δεν είστε ακόμα experts σε regex, πρόκειται για έναν τρόπο να πραγματοποιούμε συγκεκριμένες αναζητήσεις μέσα στο κείμενο. \n",
    "\n",
    "Μια μηχανή regex αντιλαμβάνεται το search pattern, στην παραπάνω περίπτωση το `'\\[\\d+\\]'` και κάποιο string από τα paragraph texts που έχουμε ορίσει. Στην συνέχεια τσεκάρει ένα ένα τα γράμματα από το  input string για να ελέγξει αν ταιριάζουν στην αναζήτηση. Εδώ το regex `'\\d'` ταιρίαζει αριθμούς ( (digits), ενώ το `'\\['` και το  `'\\]'` \"πιάνει\" τις αγκύλες σε κάθε πλευρά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNumber = r'\\d'\n",
    "regexResults = re.search(findNumber, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "regexResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στην Python το regex package (`re`) συνήθως επιστρέφει `Match` objects (μπορεί να έχουμε διάφορα pattern hits σε ένα μόνο `Match`), για να πάρουμε το string που ταίριαξε στο pattern που ζητήσαμε μπορούμε να χρησιμοποιήσουμε τη μέθοδο `.group()`. Επειδή θέλουμε το πρώτο θα ζητήσουμε το  0'. Στην πληροφορική ΠΑΝΤΑ το πρώτο είναι το 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Πήραμε τον πρώτο αριθμό, αν θέλαμε όλο το block αριθμών θα προσθέταμε ένα `'+'` , το οποίο ζητά ένα ή παραπάνω εμφανίσεις του   προηγούμενου αριθμού. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNumbers = r'\\d+'\n",
    "regexResults = re.search(findNumbers, 'όχι αρθμός, όχι αρθμός, , αριθμοί 2134567890, όχι αρθμός ')\n",
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα έχουμε όλο το block των αριθμών. \n",
    "\n",
    "Αν θέλετε να μάθετε περισσότερα για τα Python  regex δείτε [re docs](https://docs.python.org/3/library/re.html) και το μικρό \n",
    "[tutorial](https://docs.python.org/3/howto/regex.html#regex-howto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Άσκηση 1</span>\n",
    "<span style=\"color:red\"> Ακριβώς κάτω από αυτό το κελί προσθέστε νέα κελιά στα οποία θα περιγράφετε και στην συνέχεια θα κατεβάζετε περιεχόμενο από το διαδίκτυο που θα σχετίζεται με το τελικό σας project. Χρησιμοποιήστε beautiful soup και τουλάχιστον 5  regular expressions για να εξάγετε σχετικό και καθαρό κείμενο χωρίς κενά, αριθμούς που δεν χρειάζετε κ.λπ. Στην συνέχεια σώστε τις καθαρές προτάσεις κειμένου σε ένα pandas `Dataframe`.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Αράχνες (Spidering)\n",
    "\n",
    "Τι θα κάναμε αν θέλαμε διαφορετικές σελίδες από την wikipedia. Θα χρειαζόμασταν όλες τις διαφορετικές url για την καθεμιά από αυτές. Αυτό που μπορούμε να κάνουμε είναι να βρούμε σελίδες που συνδέονται με άλλες σελίδες μέσω κάποιου link. Ας προσπαθήσουμε να βρούμε τι  links υπάρχουν στην σελίδα που αναφέρεται στο content analysis.\n",
    "\n",
    "Για να προχωρήσουμε πρέπει πρώτα να αναζητήσουμε όλα τα `<a>` (anchor) tags με `href`\n",
    "(hyperlink references) που υπάρχουν μέσα στα `<p>` tags. Το `href` μπορεί να έχει διάφορες \n",
    "(http://stackoverflow.com/questions/4855168/what-is-href-and-why-is-it-used) μορφές (https://en.wikipedia.org/wiki/Hyperlink#Hyperlinks_in_HTML), οπότε πρέπει να είστε προσεκτικοί.\n",
    "\n",
    "Σε γενικές γραμμές θέλουμε να εξάγουμε ολόκληρα (absolute) ή σχετικά (relative) links. Ένα absolute link είναι αυτό που το ακολουθούμε απευθείας χωρίς μετατροπή, ενώ το relative link πρέπει πρώτα να ενωθεί με την βάση (base url) με append. Το Wikipedia χρησιμοποιεί relative urls για τα εσωτερικά links: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "otherPAgeURLS = []\n",
    "#Θέλουμε επίσης να ξέρουμε από ποθ προήλθαν τα links, άρα θέλουμε:\n",
    "#αριθμό παραγράφου\n",
    "#τη λέξη στην οποία βρίσκεται τo link\n",
    "for paragraphNum, pTag in enumerate(contentPTags):\n",
    "    # μόνο τα hrefs που οδηγούν σε wiki pages\n",
    "    tagLinks = pTag.findAll('a', href=re.compile('/wiki/'), class_=False)\n",
    "    for aTag in tagLinks:\n",
    "        #Πρέπει να εξάγουμε το url από το <a> tag\n",
    "        relurl = aTag.get('href')\n",
    "        linkText = aTag.text\n",
    "        #το wikipedia_base_url είναι η βάση στην οποία θα ενώσουμε με το urllib τα links\n",
    "        otherPAgeURLS.append((\n",
    "            urllib.parse.urljoin(wikipedia_base_url, relurl),\n",
    "            paragraphNum,\n",
    "            linkText,\n",
    "        ))\n",
    "print(otherPAgeURLS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα προσθέσουμε αυτά τα 2 νέα στοιχεία στο DataFrame `contentParagraphsDF`, άρα πρέπει να προσθέσουμε  2 ακόμη στήλες για paragraph numbers και sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentParagraphsDF['source'] = [wikipedia_content_analysis] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['paragraph-number'] = range(len(contentParagraphsDF['paragraph-text']))\n",
    "\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μετά προσθέτουμε 2 νέες στήλες στο `Dataframe` και ορίζουμε ένα function για να φέρει κάθε linked page και να προσθέσει το κείμενο της στο DataFrame μας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentParagraphsDF['source-paragraph-number'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['source-paragraph-text'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "\n",
    "def getTextFromWikiPage(targetURL, sourceParNum, sourceText):\n",
    "    #Φτιάχνουμε ένα dict στο οποίο θα σώσουμε τα data πριν τα αποθηκεύσουμε στο DataFrame\n",
    "    parsDict = {'source' : [], 'paragraph-number' : [], 'paragraph-text' : [], 'source-paragraph-number' : [],  'source-paragraph-text' : []}\n",
    "    #Τώρα παίρνουμε την σελίδα \n",
    "    r = requests.get(targetURL)\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    #Με το enumerating παίρνουμε τον αριθμό της παραγράφου\n",
    "    for parNum, pTag in enumerate(soup.body.findAll('p')):\n",
    "        #το ίδιο regex με πριν\n",
    "        parsDict['paragraph-text'].append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "        parsDict['paragraph-number'].append(parNum)\n",
    "        parsDict['source'].append(targetURL)\n",
    "        parsDict['source-paragraph-number'].append(sourceParNum)\n",
    "        parsDict['source-paragraph-text'].append(sourceText)\n",
    "    return pandas.DataFrame(parsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Και το τρέχουμε στην λίστα με τα link tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for urlTuple in otherPAgeURLS[:3]:\n",
    "    #Το ignore_index σημαίνει ότι οι δείκτες δεν θα διαγράφονται μετά από κάθε append\n",
    "    contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:red\">Άσκηση 2</span>\n",
    "<span style=\"color:red\"> Ακριβώς κάτω από αυτό το κελί προσθέστε νέα κελιά στα οποία θα φτιάξετε μια αράχνη (spider) για μία άλλη ιστοσελίδα σχετική με το τελικό σας project. Πιο συγκεκριμένα βρείτε urls στην βασική σελίδα και στην συνέχεια ακολουθήστε τα και εξάγετε το περιεχόμενο τους σε ένα pandas Dataframe. </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API (Tumblr)\n",
    "\n",
    "Οι ιδιοκτήτες των websites δεν θέλουν να κάνετε scraping στα sites τους. Αν δεν είστε προσεκτικοί και 'χτυπάτε' συνεχόμενα το site, αυτό θα μοιάζει με DOS attack. Ορισμένα sites θέλουν αυτοματοποιημένα  εργαλεία για να έχουμε πρόσβαση στα δεδομένα τους και οι ίδιοι δημιουργούν [application programming interface\n",
    "(APIs)](https://en.wikipedia.org/wiki/Application_programming_interface). Ένα API\n",
    "συγκκριμενοποιεί τη διαδικασία με την οποία παρέχει τις πληροφορίες. Συνήθως γίνεται μέσω μιας [representational state transfer\n",
    "(REST)](https://en.wikipedia.org/wiki/Representational_state_transfer) web\n",
    "service.\n",
    "\n",
    "Ένα καλό παράδειγμα για εμάς είναι το [Tumblr](https://www.tumblr.com), που μας παρέχουν \n",
    "[simple RESTful API](https://www.tumblr.com/docs/en/api/v1) το οποίο μας επιτρέπει να διαβάζουμε posts χωρίς να χρειάζεται να κάνουμε δύσκολο html parsing.\n",
    "\n",
    "Μπορούμε εύκολα να πάρουμε τα πρώτα 20 posts από ένα blog μέσω μιας http GET request στο\n",
    "`'http://{blog}.tumblr.com/api/read/json'`, το `{blog}` είναι το όνομα του blog που στοχεύουμε. Ας πάρουμε μερικές δημοσιεύσεις από εδώ [http://lolcats-lol-\n",
    "cat.tumblr.com/](http://lolcats-lol-cat.tumblr.com/) (Το όνομα του Tumblr blog βρίσκεται στο URL 'lolcats-lol-cat')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'))\n",
    "\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορεί να μην φαίνεται εύκολο, αλλά είναι πολύ ευκολότερο από την html! Αυτό που βλέπουμε είναι ένα\n",
    "[JSON](https://en.wikipedia.org/wiki/JSON) ένα 'human readable' κείμενο βασισμένο στα δεδομένα που στάλθηκαν σε ένα transmission format βασισμένο στην γλώσσα javascript. Ευτυχώς για εμάς, μπορούμε εύκολα να το μετατρέψουμε σε ένα python `dictionary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "print(d.keys())\n",
    "print(len(d['posts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αν διαβάσουμε το [API specification](https://www.tumblr.com/docs/en/api/v1), θα δούμε ότι μπορούμε να πάρουμε πολλά πράγματα αν δwe\n",
    "will see there are a lot of things we can get if we add things to our GET\n",
    "request. First we can retrieve posts by their id number. Let's first get post\n",
    "`146020177084`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'), params = {'id' : 146020177084})\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "d['posts'][0].keys()\n",
    "d['posts'][0]['photo-url-1280']\n",
    "\n",
    "with open('lolcat.gif', 'wb') as f:\n",
    "    gifRequest = requests.get(d['posts'][0]['photo-url-1280'], stream = True)\n",
    "    f.write(gifRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='lolcat.gif'>\n",
    "\n",
    "Αν δεν μπορείτε να το δείτε κάντε refresh. Τώρα μπορούμε να πάρουμε το κείμενο από όλα τα posts μαζί με τα σχετικά metadata, όπως το post date, το caption και τα tags. Επίσης μπορούμε να πάρουμε τα \n",
    "links των φωτογραφιών."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Βάζουμε έναν αριθμό max σε περίπτωση που το blog έχει εκατομμύρια εικόνες.\n",
    "#Tο max θα είναι πολλαπλάσιο του 50\n",
    "def tumblrImageScrape(blogName, maxImages = 200):\n",
    "    tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "   \n",
    "    possiblePhotoSuffixes = [1280, 500, 400, 250, 100]\n",
    "\n",
    "    #Όλες τις πληροφορίες θα τις αποθηκεύσουμε στο τέλος σε ένα DataFrame.\n",
    "    #Στο Tumblr documentation μπορείτε να βρείτε πώς θα πάρετε και άλλες πληροφροίες.\n",
    "    #https://www.tumblr.com/docs/en/api/v1\n",
    "    postsData = {\n",
    "        'id' : [],\n",
    "        'photo-url' : [],\n",
    "        'date' : [],\n",
    "        'tags' : [],\n",
    "        'photo-type' : []\n",
    "    }\n",
    "\n",
    "    #Το Tumblr μας περιορίζει σε max 50 posts σε κάθε request\n",
    "    for requestNum in range(maxImages // 50):\n",
    "        requestParams = {\n",
    "            'start' : requestNum * 50,\n",
    "            'num' : 50,\n",
    "            'type' : 'photo'\n",
    "        }\n",
    "        r = requests.get(tumblrAPItarget.format(blogName), params = requestParams)\n",
    "        requestDict = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "        for postDict in requestDict['posts']:\n",
    "            #Παίρνουμε uncleaned data, δεν μπορούμε να τα εμπιστευτούμε.\n",
    "            #Δηλαδή, όλα τα posts δεν είναι απαραίτητο ότι περιέχουν όλες τις πληροφορίες που ζητάμε να πάρουμε.\n",
    "            try:\n",
    "                postsData['id'].append(postDict['id'])\n",
    "                postsData['date'].append(postDict['date'])\n",
    "                postsData['tags'].append(postDict['tags'])\n",
    "            except KeyError as e:\n",
    "                raise KeyError(\"Post {} from {} is missing: {}\".format(postDict['id'], blogName, e))\n",
    "\n",
    "            foundSuffix = False\n",
    "            for suffix in possiblePhotoSuffixes:\n",
    "                try:\n",
    "                    photoURL = postDict['photo-url-{}'.format(suffix)]\n",
    "                    postsData['photo-url'].append(photoURL)\n",
    "                    postsData['photo-type'].append(photoURL.split('.')[-1])\n",
    "                    foundSuffix = True\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if not foundSuffix:\n",
    "                #Διαβάστε τα λάθη\n",
    "                raise KeyError(\"Post {} from {} is missing a photo url\".format(postDict['id'], blogName))\n",
    "\n",
    "    return pandas.DataFrame(postsData)\n",
    "tumblrImageScrape('lolcats-lol-cat', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα που έχουμε πολλά urls από φωτογραφίες θα μπορούσαμε να δοκιμάσουμε OCR και ίσως βρούμε μοτίβα στα memes με τις γάτες.\n",
    "\n",
    "# Αρχεία\n",
    "\n",
    "Όταν το κείμενο που αναζητούμε δεν βρίσκεται στο διαδίκτυο, τότε θα είναι σε μια μορφή αρχείου (*files*).\n",
    "\n",
    "## Raw αρχεία κειμένου (και κωδικοποίηση)\n",
    "\n",
    "Η τελείως βασική μορφή αποθήκευση ενός κειμένου είναι ως _raw text_ document. Ο πηγαίος κώδικας \n",
    "(`.py`, `.r`, κ.λπ) είναι συνήθως raw text όπως το (`.txt`) και άλλες επεκτάσεις όπως (.csv, .dat, κ.λπ.). Για να δούμε τι μορφής είναι ένα άγνωστο κείμενο μπορούμε να το ανοίξουμε με έναν \n",
    "text editor.\n",
    "\n",
    "Στην python δημιουργούμε ένα αρχείο κειμένου χρησιμοποιώντας το function `open()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_text_file = 'sometextfile.txt'\n",
    "#stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols \\u2421 \\u241B \\u20A0 \\u20A1 \\u20A2 \\u20A3 \\u0D60\\n'\n",
    "stringToWrite = 'A line\\nAnother line\\nA line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\\n'\n",
    "\n",
    "with open(example_text_file, mode = 'w', encoding='utf-8') as f:\n",
    "    f.write(stringToWrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το `encoding='utf-8'` ορίζει τον τρόπο που εμφανίζονται οι χαρακτήρες [ASCII](https://en.wikipedia.org/wiki/ASCII) και εμφανίζει 128 χαρακτήρες. Στα αγγλικά και σε άλλες γλώσσες που χρησιμοποιούν το λατινικό αλφάβητο δεν υπάρχει κάποιο πρόβλημα, αλλά σε άλλες γλώσσες όπως τα ελληνικά ή τα κινέζικα πρέπει η κωδικοποίηση να είναι συμβατή και χρησιμοποιούμε το \n",
    "[Unicode](https://en.wikipedia.org/wiki/Unicode) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(example_text_file, encoding='utf-8') as f:\n",
    "    print(\"This is with the correct encoding:\")\n",
    "    print(f.read())\n",
    "\n",
    "with open(example_text_file, encoding='latin-1') as f:\n",
    "    print(\"This is with the wrong encoding:\")\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρήστε ότι στο _latin-1_ οι unicode χαρακτήρες εμφανίζονται μπερδεμένοι και είναι πολλοί. Πρέπει πάντα να ελέγχετε ότι δουλέυετε με το σωστό encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορούμε να εισάγουμε πολλά αρχεία μαζί. Ας ρίξουμε μια ματιά στα κείμενα του Shakespeare που βρίσκονται στο `data` directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/Shakespeare/midsummer_nights_dream.txt') as f:\n",
    "    midsummer = f.read()\n",
    "print(midsummer[-700:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Προκειμένου να φορτώσουμε όλα τα αρχεία από το `../data/Shakespeare` μπορούμε να φτιάξουμε ένα for loop με `scandir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDir = '../data/Shakespeare' #Αλλάξτε το σύμφωνα με το δικό σας directory των κειμένων\n",
    "shakespearText = []\n",
    "shakespearFileName = []\n",
    "\n",
    "for file in (file for file in os.scandir(targetDir) if file.is_file() and not file.name.startswith('.')):\n",
    "    with open(file.path) as f:\n",
    "        shakespearText.append(f.read())\n",
    "    shakespearFileName.append(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ας τα βάλουμε σε ένα pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespear_df = pandas.DataFrame({'text' : shakespearText}, index = shakespearFileName)\n",
    "shakespear_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η αποθήκευση του κειμένου σε αυτή τη μορφή αποτελεί το πρώτο βήμα για την ανάλυση.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF\n",
    "\n",
    "Επίσης πολύ συχνά τα κείμενα βρίσκονται σε μορφή PDF και είναι αναρτημένα στο διαδίκτυο. Πρώτα πρέπει να τα κατεβάσουμε τοπικά με την Python. Ας δοκιμάσουμε να κατεβάσουμε ένα κεφάλαιο από το βιβλίο\n",
    "Speech and Language Processing_. Το chapter 21 αναλύει την έννοια του Information Extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#information_extraction_pdf = 'https://github.com/cathrinesot/raw/blob/master/21.pdf'\n",
    "\n",
    "infoExtractionRequest = requests.get(information_extraction_pdf, stream=True)\n",
    "print(infoExtractionRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Γράφει `'pdf'`, το οποίο είναι καλό σημάδι! Το υπόλοπο φαίνεται να έχει προβλήματα με το encoding. Αυτά που εμφανίζονται όμως δεν είναι πρόβλημα στο encoding, γιατί δεν υπάρχει κάποιο encoding για αυτά τα κομμάτια. Αυτό που χρειαζόμαστε είναι κάτι που να γνωρίζει από pdf και πώς να τα διαβάσει. Για αυτό θα χρησιμοποιήσουμε το\n",
    "[`PyPDF2`](https://github.com/mstamy2/PyPDF2), μια PDF processing library για\n",
    "Python 3.\n",
    "\n",
    "\n",
    "Ο παρακάτω κώδικας είναι ένα function το οποίο παίρνει το PDF file και επιστρέφει το κείμενο και θα το χρησιμοποιείτε αυτούσιο από δω και πέρα. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPDF(pdfFile):\n",
    "    #Bασισμένο στον κώδικα http://stackoverflow.com/a/20905381/4955164\n",
    "    #Χρήση του utf-8\n",
    "    codec = 'utf-8'\n",
    "    rsrcmgr = pdfminer.pdfinterp.PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    layoutParams = pdfminer.layout.LAParams()\n",
    "    device = pdfminer.converter.TextConverter(rsrcmgr, retstr, laparams = layoutParams, codec = codec)\n",
    "    #Χρειαζόμαστε έναν interpreter\n",
    "    interpreter = pdfminer.pdfinterp.PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in pdfminer.pdfpage.PDFPage.get_pages(pdfFile, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    device.close()\n",
    "    returnedString = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return returnedString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Πρώτα χρειάζεται να πάρουμε το response object και να το μετατρέψουμε σε κάτι που να μοιάζει με αρχείο ώστε να μπορεί το pdfminer να το διαβάσει. Θέλουμε άρα από τη βιβλιοθήκη `io`', το `BytesIO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoExtractionBytes = io.BytesIO(infoExtractionRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα ας το δώσουμε στο pdfminer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(readPDF(infoExtractionBytes)[:550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Docs\n",
    "\n",
    "Ένας άλλο είδος αποθήκευσης κειμένου είναι το `.docx`, και πρόκειται για μια μορφή [XML](https://en.wikipedia.org/wiki/Office_Open_XML), και όπως στην HTML, χρειαζόμαστε ειδικό parser [`python-docx`](https://python-docx.readthedocs.io/en/latest/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_docx = 'https://github.com/KnowledgeLab/content_analysis/raw/data/example_doc.docx'\n",
    "\n",
    "r = requests.get(example_docx, stream=True)\n",
    "d = docx.Document(io.BytesIO(r.content))\n",
    "for paragraph in d.paragraphs[:7]:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Χρειαζόμαστε ξανά το `io.BytesIO`, καθώς το `docx.Document` περιμένει να βρει ένα αρχείο. Ένας άλλος τρόπος είναι να σώσουμε το αρχείο και μετά να το εισάγουμε.  Αν το κάνουμε αυτό μπορούμε να σβήσουμε το αρχείο όταν τελειλωσουμε, ή να το σώσουμε τοπικά. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadIfNeeded(targetURL, outputFile, **openkwargs):\n",
    "    if not os.path.isfile(outputFile):\n",
    "        outputDir = os.path.dirname(outputFile)\n",
    "        #Πιο general os.mkdir()\n",
    "        if len(outputDir) > 0:\n",
    "            os.makedirs(outputDir, exist_ok = True)\n",
    "        r = requests.get(targetURL, stream=True)\n",
    "        #Βάζουμε το κλείσιμο του αρχείου \n",
    "        with open(outputFile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    return open(outputFile, **openkwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό το function θα κατεβάσει, θα σώσει και θα ανοίξει το `outputFile` ως `outputFile` ή απλά θα το ανοίξει αν το `outputFile` υπάρχει. Το `open()` θα ανοίξει το αρχείο read only text με το τοπικό  encoding, το οποίο μπορεί να δημιουργήσει προβλήματα σε περίπτωση που το αρχείο δεν περιέχει κείμενο. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    d = docx.Document(downloadIfNeeded(example_docx, example_docx_save))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Πρέπει να πούμε `open()` για να διαβαστεί σε binary mode (`'rb'`), γιαυτό προσθέσαμε το \n",
    "`**openkwargs`, μας επιτρέπει να βάλουμε  keyword arguments (kwargs) από το \n",
    "`downloadIfNeeded` στο `open()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = docx.Document(downloadIfNeeded(example_docx, example_docx_save, mode = 'rb'))\n",
    "for paragraph in d.paragraphs[:7]:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα μπορούμε να διαβάσουμε το αρχείο `docx.Document` και να μην περίμενουμε να ξανακατέβει την επόμενη φορά."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:red\">Άσκηση 3</span>\n",
    "<span style=\"color:red\"> Ακριβώς κάτω από αυτό το κελί προσθέστε νέα κελιά στα οποία θα εξάγετε και θα οργανώσετε κείμενα από αρχεία text, PDF or Word σε ένα pandas Dataframe.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
