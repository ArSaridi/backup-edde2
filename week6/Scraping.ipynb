{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Εβδομάδα 4 - Ανάκτηση και προετοιμασία δεδομένων\n",
    "\n",
    "\n",
    "\n",
    "Στο συγκεκριμένο notebook θα εξετάσουμε με ποιον τρόπο μπορούμε να ανακτήσουμε δεδομένα που βρίσκονται αναρτημένα στο διαδίκτυο (scraping), σε μορφή HTML. Στην συνέχεια θα δοκιμάσουμε να περιηγηθούμε μέσω υπερσυνδέσεων για να δημιουργήσουμε ένα δείγμα από online περιεχόμενο, με την τεχνική \"spidering\" ή με την χρήση APIs, (Application Programming Interfaces), που παρέχονται ορισμένες φορές από τις διάφορες εταιρείες σε προγραμματιστές και παρέχουν πρόσβαση στο περιεχόμενό τους. \n",
    "\n",
    "Στην συνέχεια των μαθημάτων θα δοκιμάσουμε regular expressions, για να \"καθαρίσουμε\" τα κείμενα από ανεπιθύμητο περιεχόμενο όπως σημεία στίξης, μορφοποιήσεις κ.λπ.\n",
    "\n",
    "Στο τέλος θα εξερευνήσουμε τους τρόπους με τους οποίους μπορούμε να αποθηκέυσουμε τα δεδομένα, ώστε να είναι έτοιμα για ανάλυση. \n",
    "\n",
    "Για το συγκεκριμένο notebook θα χρειαστούμε τα εξής πακέτα: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Τα παρακάτω πακέτα θα εγκατασταθούν με την εντολή pip στον υπολογιστή (στον σέρβερ υπάρχουν ήδη)\n",
    "\n",
    "import requests #βιβλιοθήκη για http requests\n",
    "import bs4 #ονομάζεται `beautifulsoup4`, και είναι ένας html parser\n",
    "import pandas #δημιουργεί DataFrames\n",
    "import docx #διαβάζει MS doc files, για εγκατάσταση θέλουμε το `python-docx`\n",
    "\n",
    "#Tα παρακάτω υπάρχουν στην Python\n",
    "import re #regexs\n",
    "import urllib.parse #ένωση urls\n",
    "import io #κάνει τα http requests να μοιάζουν με αρχεία\n",
    "import json #API responses\n",
    "import os.path #Τσεκάρει ότι υπάρχουν τα αρχεία \n",
    "import os #Δημιουργεί directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα δουλέψουμε επίσης με τα παρακάτω αρχεία και urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "example_text_file = 'sometextfile.txt'\n",
    "information_extraction_pdf = 'https://github.com/Computational-Content-Analysis-2018/Data-Files/raw/master/1-intro/Content%20Analysis%2018.pdf'\n",
    "example_docx = 'https://github.com/Computational-Content-Analysis-2018/Data-Files/raw/master/1-intro/macs6000_connecting_to_midway.docx'\n",
    "example_docx_save = 'example.docx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "Πριν ξεκινήσει η ανάλυση περιεχομένου πρέπει να έχουμε το περιεχόμενο στην κατοχή μας. Μερικές φορές μπορεί να φτάσει στα χέρια μας ήδη \"καθαρό\" και έτοιμο για χρήση ως αρχείο κειμένου και άλλες φορές θα χρειαστεί να το κατεβάσουμε από το διαδίκτυο.\n",
    "\n",
    "Για αρχή ας δοκιμάσουμε να κατεβάσουμε μια σελίδα από το wikipedia για ανάλυση περιεχομένου. \n",
    "\n",
    "Η σελίδα βρίσκεται στο (https://en.wikipedia.org/wiki/Content_analysis)\n",
    "\n",
    "Θα χρησιμοποιήσουμε μία HTTP GET request για το συγκεκριμένο url, μια GET request είναι ένα απλό αίτημα στον server για να μας δώσει το περιεχόμενο αυτού του url. Ένα άλλο είδος είναι το POST request και πρόκειται για ένα αίτημα στο οποίο ζητάμε εμείς από τον server να πάρει ένα αρχείο που του δίνουμε. \n",
    "\n",
    "Παρόλο που η Python έχει ήδη μια βιβλιοθήκη για GET requests, εμείς θα χρησιμοποιούμε την \n",
    "[_requests_](http://docs.python-requests.org/en/master/) γιατί είναι πιο εύχρηστη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'\n",
    "requests.get(wikipedia_content_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'Response [200]'` σημαίνει ότι ο server ανταποκρίθηκε σε αυτό που του ζητήσαμε. Αν παίρναμε άλλον αριθμό (π.χ. 404) θα σήμαινε ότι υπάρχει κάποιο error. \n",
    "\n",
    "Υπάρχει λίστα με όλους τους διαθέσιμους κωδικούς από HTTP response codes εδώ: \n",
    "[here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes). \n",
    "\n",
    "To response διαθέτει όλα τα data που μας έστειλε ο server, το περιεχόμενο του website και το HTTP header. Εμάς μας ενδιαφέρει το περιεχόμενο που μπορούμε να το προσπελάσουμε με την εντολή `.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiContentRequest = requests.get(wikipedia_content_analysis)\n",
    "print(wikiContentRequest.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό δεν είναι το αποτέλεσμα που περιμέναμε, καθώς πρόκειται για την αρχή της  HTML του website. Η γλώσσα αυτή διαβάζεται από υπολογιστές, για το λόγο αυτό θα χρησιμοποιήσουμε έναν parser για να τη διαβάσουμε. Για το parsing θα χρησιμοποιήσουμε το [_Beautiful\n",
    "Soup_](https://www.crummy.com/software/BeautifulSoup/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiContentSoup = bs4.BeautifulSoup(wikiContentRequest.text, 'html.parser')\n",
    "print(wikiContentSoup.text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό είναι καλύτερο από το προηγούμενο αποτέλεσμα, αλλά και πάλι περιέχει πολλά κενά και όχι μόνο κείμενο. Αυτό συνέβη γιατί ζητήσαμε όλη την webpage, και όχι μόνο το κείμενο.\n",
    "\n",
    "Εμείς θέλουμε μόνο το κείμενο και για να πετύχουμε να πάρουμε μόνο αυτό, πρέπει να ελέγξουμε την html. Ένας εύκολος τρόπος να το κάνουμε αυτό είναι να επισκεφθούμε το website με έναν browser και να χρησιμοποιήσουμε το inspection ή το \"view source tool\". Σε περίπτωση που υπάρχει javascript ή κάποιο άλλο είδος δυναμικού loading στην σελίδα, τότε πρέπει να ελέγξουμε αυτό που λαμβάνει η Python και χρειαζόμαστε τη βιβλιοθήκη `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content_analysis_save = 'wikipedia_content_analysis.html'\n",
    "\n",
    "with open(content_analysis_save, mode='w', encoding='utf-8') as f:\n",
    "    f.write(wikiContentRequest.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα ας ανοίξουμε το αρχείο (`wikipedia_content_analysis.html`) που μόλις δημιουργήσαμε με τον web browser. Πρέπει να μοιάζει όπως το original αλλά χωρίς τις εικόνες και τη μορφοποίηση. \n",
    "\n",
    "Επειδή δεν υπάρχει τίποτα θέσφατο στη δημιουργία μιας ιστοσελίδας, το να καταλάβουμε τι πραγματικά είναι χρήσιμο για μας είναι Τέχνη. Κοιτώντας σε αυτή την σελίδα φαίνεται πως το κυρίως κείμενο είναι μέσα στα `<p>`(paragraph) tags που βρίσκονται μέσα στο `<body>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentPTags = wikiContentSoup.body.findAll('p')\n",
    "for pTag in contentPTags[:3]:\n",
    "    print(pTag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα έχουμε όλο το κείμενο χωρισμένο σε παραγράφους.  \n",
    "\n",
    "Μας μένει να κάνουμε κάτι ακόμα πριν αρχίσουμε την επεξεργασία, να βγάλουμε τα references indicators (`[2]`, `[3]` , κ.λπ). Για να γίνει αυτό χρειάζεται λίγο regular expression (regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentParagraphs = []\n",
    "for pTag in contentPTags:\n",
    "    contentParagraphs.append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "\n",
    "#μετατροπή σε DataFrame\n",
    "contentParagraphsDF = pandas.DataFrame({'paragraph-text' : contentParagraphs})\n",
    "print(contentParagraphsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα έχουμε ένα `DataFrame` που περιέχει όλα τα σχετικά κείμενα από την ιστοσελίδα. \n",
    "\n",
    "Επειδή δεν είστε ακόμα experts σε regex, πρόκειται για έναν τρόπο να πραγματοποιούμε συγκεκριμένες αναζητήσεις μέσα στο κείμενο. \n",
    "\n",
    "Μια μηχανή regex αντιλαμβάνεται το search pattern, στην παραπάνω περίπτωση το `'\\[\\d+\\]'` και κάποιο string από τα paragraph texts που έχουμε ορίσει. Στην συνέχεια τσεκάρει ένα ένα τα γράμματα από το  input string για να ελέγξει αν ταιριάζουν στην αναζήτηση. Εδώ το regex `'\\d'` ταιρίαζει αριθμούς ( (digits), ενώ το `'\\['` και το  `'\\]'` \"πιάνει\" τις αγκύλες σε κάθε πλευρά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNumber = r'\\d'\n",
    "regexResults = re.search(findNumber, 'not a number, not a number, numbers 2134567890, not a number')\n",
    "regexResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στην Python το regex package (`re`) συνήθως επιστρέφει `Match` objects (μπορεί να έχουμε διάφορα pattern hits σε ένα μόνο `Match`), για να πάρουμε το string που ταίριαξε στο pattern που ζητήσαμε μπορούμε να χρησιμοποιήσουμε τη μέθοδο `.group()`. Επειδή θέλουμε το πρώτο θα ζητήσουμε το  0'. Στην πληροφορική ΠΑΝΤΑ το πρώτο είναι το 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Πήραμε τον πρώτο αριθμό, αν θέλαμε όλο το block αριθμών θα προσθέταμε ένα `'+'` , το οποίο ζητά ένα ή παραπάνω εμφανίσεις του   προηγούμενου αριθμού. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNumbers = r'\\d+'\n",
    "regexResults = re.search(findNumbers, 'όχι αρθμός, όχι αρθμός, , αριθμοί 2134567890, όχι αρθμός ')\n",
    "print(regexResults.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τώρα έχουμε όλο το block των αριθμών. \n",
    "\n",
    "Αν θέλετε να μάθετε περισσότερα για τα Python  regex δείτε [re docs](https://docs.python.org/3/library/re.html) και το μικρό \n",
    "[tutorial](https://docs.python.org/3/howto/regex.html#regex-howto).\n",
    "\n",
    "Το καλύτερο tutorial βρίσκεται εδώ: https://regexone.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Αράχνες (Spidering)\n",
    "\n",
    "Τι θα κάναμε αν θέλαμε διαφορετικές σελίδες από την wikipedia. Θα χρειαζόμασταν όλες τις διαφορετικές url για την καθεμιά από αυτές. Αυτό που μπορούμε να κάνουμε είναι να βρούμε σελίδες που συνδέονται με άλλες σελίδες μέσω κάποιου link. Ας προσπαθήσουμε να βρούμε τι  links υπάρχουν στην σελίδα που αναφέρεται στο content analysis.\n",
    "\n",
    "Για να προχωρήσουμε πρέπει πρώτα να αναζητήσουμε όλα τα `<a>` (anchor) tags με `href`\n",
    "(hyperlink references) που υπάρχουν μέσα στα `<p>` tags. Το `href` μπορεί να έχει διάφορες \n",
    "(http://stackoverflow.com/questions/4855168/what-is-href-and-why-is-it-used) μορφές (https://en.wikipedia.org/wiki/Hyperlink#Hyperlinks_in_HTML), οπότε πρέπει να είστε προσεκτικοί.\n",
    "\n",
    "Σε γενικές γραμμές θέλουμε να εξάγουμε ολόκληρα (absolute) ή σχετικά (relative) links. Ένα absolute link είναι αυτό που το ακολουθούμε απευθείας χωρίς μετατροπή, ενώ το relative link πρέπει πρώτα να ενωθεί με την βάση (base url) με append. Το Wikipedia χρησιμοποιεί relative urls για τα εσωτερικά links: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikipedia_base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "otherPAgeURLS = []\n",
    "#Θέλουμε επίσης να ξέρουμε από που προήλθαν τα links, άρα θέλουμε:\n",
    "#αριθμό παραγράφου\n",
    "#τη λέξη στην οποία βρίσκεται τo link\n",
    "for paragraphNum, pTag in enumerate(contentPTags):\n",
    "    # μόνο τα hrefs που οδηγούν σε wiki pages\n",
    "    tagLinks = pTag.findAll('a', href=re.compile('/wiki/'), class_=False)\n",
    "    for aTag in tagLinks:\n",
    "        #Πρέπει να εξάγουμε το url από το <a> tag\n",
    "        relurl = aTag.get('href')\n",
    "        linkText = aTag.text\n",
    "        #το wikipedia_base_url είναι η βάση στην οποία θα ενώσουμε με το urllib τα links\n",
    "        otherPAgeURLS.append((\n",
    "            urllib.parse.urljoin(wikipedia_base_url, relurl),\n",
    "            paragraphNum,\n",
    "            linkText,\n",
    "        ))\n",
    "print(otherPAgeURLS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα προσθέσουμε αυτά τα 2 νέα στοιχεία στο DataFrame `contentParagraphsDF`, άρα πρέπει να προσθέσουμε  2 ακόμη στήλες για paragraph numbers και sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentParagraphsDF['source'] = [wikipedia_content_analysis] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['paragraph-number'] = range(len(contentParagraphsDF['paragraph-text']))\n",
    "\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μετά προσθέτουμε 2 νέες στήλες στο `Dataframe` και ορίζουμε ένα function για να φέρει κάθε linked page και να προσθέσει το κείμενο της στο DataFrame μας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentParagraphsDF['source-paragraph-number'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "contentParagraphsDF['source-paragraph-text'] = [None] * len(contentParagraphsDF['paragraph-text'])\n",
    "\n",
    "def getTextFromWikiPage(targetURL, sourceParNum, sourceText):\n",
    "    #Φτιάχνουμε ένα dict στο οποίο θα σώσουμε τα data πριν τα αποθηκεύσουμε στο DataFrame\n",
    "    parsDict = {'source' : [], 'paragraph-number' : [], 'paragraph-text' : [], 'source-paragraph-number' : [],  'source-paragraph-text' : []}\n",
    "    #Τώρα παίρνουμε την σελίδα \n",
    "    r = requests.get(targetURL)\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    #Με το enumerating παίρνουμε τον αριθμό της παραγράφου\n",
    "    for parNum, pTag in enumerate(soup.body.findAll('p')):\n",
    "        #το ίδιο regex με πριν\n",
    "        parsDict['paragraph-text'].append(re.sub(r'\\[\\d+\\]', '', pTag.text))\n",
    "        parsDict['paragraph-number'].append(parNum)\n",
    "        parsDict['source'].append(targetURL)\n",
    "        parsDict['source-paragraph-number'].append(sourceParNum)\n",
    "        parsDict['source-paragraph-text'].append(sourceText)\n",
    "    return pandas.DataFrame(parsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Και το τρέχουμε στη λίστα με τα link tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for urlTuple in otherPAgeURLS[:3]:\n",
    "    #Το ignore_index σημαίνει ότι οι δείκτες δεν θα διαγράφονται μετά από κάθε append\n",
    "    contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)\n",
    "contentParagraphsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API (Tumblr)\n",
    "\n",
    "Οι ιδιοκτήτες των websites δεν θέλουν να κάνετε scraping στα sites τους. Αν δεν είστε προσεκτικοί και 'χτυπάτε' συνεχόμενα το site, αυτό θα μοιάζει με DOS attack. Ορισμένα sites θέλουν αυτοματοποιημένα  εργαλεία για να έχουμε πρόσβαση στα δεδομένα τους και οι ίδιοι δημιουργούν [application programming interface\n",
    "(APIs)](https://en.wikipedia.org/wiki/Application_programming_interface). Ένα API\n",
    "συγκκριμενοποιεί τη διαδικασία με την οποία παρέχει τις πληροφορίες. Συνήθως γίνεται μέσω μιας [representational state transfer\n",
    "(REST)](https://en.wikipedia.org/wiki/Representational_state_transfer) web\n",
    "service.\n",
    "\n",
    "Ένα καλό παράδειγμα για εμάς είναι το [Tumblr](https://www.tumblr.com), που μας παρέχουν \n",
    "[simple RESTful API](https://www.tumblr.com/docs/en/api/v1) το οποίο μας επιτρέπει να διαβάζουμε posts χωρίς να χρειάζεται να κάνουμε δύσκολο html parsing.\n",
    "\n",
    "Μπορούμε εύκολα να πάρουμε τα πρώτα 20 posts από ένα blog μέσω μιας http GET request στο\n",
    "`'http://{blog}.tumblr.com/api/read/json'`, το `{blog}` είναι το όνομα του blog που στοχεύουμε. Ας πάρουμε μερικές δημοσιεύσεις από εδώ [http://lolcats-lol-\n",
    "cat.tumblr.com/](http://lolcats-lol-cat.tumblr.com/) (Το όνομα του Tumblr blog βρίσκεται στο URL 'lolcats-lol-cat')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'))\n",
    "\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μπορεί να μην φαίνεται εύκολο, αλλά είναι πολύ ευκολότερο από την html! Αυτό που βλέπουμε είναι ένα\n",
    "[JSON](https://en.wikipedia.org/wiki/JSON) ένα 'human readable' κείμενο βασισμένο στα δεδομένα που στάλθηκαν σε ένα transmission format βασισμένο στην γλώσσα javascript. Ευτυχώς για εμάς, μπορούμε εύκολα να το μετατρέψουμε σε ένα python `dictionary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "print(d.keys())\n",
    "print(len(d['posts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αν διαβάσουμε το [API specification](https://www.tumblr.com/docs/en/api/v1), θα δούμε ότι μπορούμε να πάρουμε πολλά πράγματα με ένα GET request. Πρώτα θα πάρουμε κάποια posts με το id number. Ας ξεκινήσουμε με το post\n",
    "`146020177084`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(tumblrAPItarget.format('lolcats-lol-cat'), params = {'id' : 146020177084})\n",
    "d = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "d['posts'][0].keys()\n",
    "d['posts'][0]['photo-url-1280']\n",
    "\n",
    "with open('lolcat.gif', 'wb') as f:\n",
    "    gifRequest = requests.get(d['posts'][0]['photo-url-1280'], stream = True)\n",
    "    f.write(gifRequest.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='lolcat.gif'>\n",
    "\n",
    "Αν δεν μπορείτε να το δείτε κάντε refresh. Τώρα μπορούμε να πάρουμε το κείμενο από όλα τα posts μαζί με τα σχετικά metadata, όπως το post date, το caption και τα tags. Επίσης μπορούμε να πάρουμε τα \n",
    "links των φωτογραφιών."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Βάζουμε έναν αριθμό max σε περίπτωση που το blog έχει εκατομμύρια εικόνες.\n",
    "#Tο max θα είναι πολλαπλάσιο του 50\n",
    "def tumblrImageScrape(blogName, maxImages = 200):\n",
    "    tumblrAPItarget = 'http://{}.tumblr.com/api/read/json'\n",
    "\n",
    "   \n",
    "    possiblePhotoSuffixes = [1280, 500, 400, 250, 100]\n",
    "\n",
    "    #Όλες τις πληροφορίες θα τις αποθηκεύσουμε στο τέλος σε ένα DataFrame.\n",
    "    #Στο Tumblr documentation μπορείτε να βρείτε πώς θα πάρετε και άλλες πληροφροίες.\n",
    "    #https://www.tumblr.com/docs/en/api/v1\n",
    "    postsData = {\n",
    "        'id' : [],\n",
    "        'photo-url' : [],\n",
    "        'date' : [],\n",
    "        'tags' : [],\n",
    "        'photo-type' : []\n",
    "    }\n",
    "\n",
    "    #Το Tumblr μας περιορίζει σε max 50 posts σε κάθε request\n",
    "    for requestNum in range(maxImages // 50):\n",
    "        requestParams = {\n",
    "            'start' : requestNum * 50,\n",
    "            'num' : 50,\n",
    "            'type' : 'photo'\n",
    "        }\n",
    "        r = requests.get(tumblrAPItarget.format(blogName), params = requestParams)\n",
    "        requestDict = json.loads(r.text[len('var tumblr_api_read = '):-2])\n",
    "        for postDict in requestDict['posts']:\n",
    "            #Παίρνουμε uncleaned data, δεν μπορούμε να τα εμπιστευτούμε.\n",
    "            #Δηλαδή, όλα τα posts δεν είναι απαραίτητο ότι περιέχουν όλες τις πληροφορίες που ζητάμε να πάρουμε.\n",
    "            try:\n",
    "                postsData['id'].append(postDict['id'])\n",
    "                postsData['date'].append(postDict['date'])\n",
    "                postsData['tags'].append(postDict['tags'])\n",
    "            except KeyError as e:\n",
    "                raise KeyError(\"Post {} from {} is missing: {}\".format(postDict['id'], blogName, e))\n",
    "\n",
    "            foundSuffix = False\n",
    "            for suffix in possiblePhotoSuffixes:\n",
    "                try:\n",
    "                    photoURL = postDict['photo-url-{}'.format(suffix)]\n",
    "                    postsData['photo-url'].append(photoURL)\n",
    "                    postsData['photo-type'].append(photoURL.split('.')[-1])\n",
    "                    foundSuffix = True\n",
    "                    break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if not foundSuffix:\n",
    "                #Διαβάστε τα λάθη\n",
    "                raise KeyError(\"Post {} from {} is missing a photo url\".format(postDict['id'], blogName))\n",
    "\n",
    "    return pandas.DataFrame(postsData)\n",
    "tumblrImageScrape('lolcats-lol-cat', 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
