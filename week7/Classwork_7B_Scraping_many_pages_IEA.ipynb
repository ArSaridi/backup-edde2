{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping: Άρθρα από το Institute of Economic Affairs (https://iea.org.uk/blog)\n",
    "\n",
    "\n",
    "### Πάμε να πάρουμε περιεχόμενο από την αρχική σελίδα του Institute of Economic Affairs. \n",
    "\n",
    "Θέλουμε:\n",
    "\n",
    "Τίτλους (Headlines)\n",
    "\n",
    "Συνόψεις (Summary)\n",
    "\n",
    "Συγγραφείς (Author)\n",
    "\n",
    "Υπερσυνδέσεις (link)\n",
    "\n",
    "Ημερομηνία δημοσίευσης (date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ξεκινώντας πρέπει να εισάγουμε τα απαραίτητα libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ορίζουμε τα headers για να νομίζει ο σέρβερ της ιστοσελίδας ότι πρόκειται για browser και όχι για εμάς!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Παίρνουμε την σελίδα και την μετατρέπουμε σε ένα αντικείμενο beautiful soup (BS object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://iea.org.uk/blog\", headers=headers)\n",
    "#response = requests.get(\"https://www.nytimes.com/\", headers=headers)\n",
    "doc = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Πάμε στην ιστοσελίδα και ελέγχουμε τι ψάχνουμε να βρούμε, για παράδειγμα εδώ θέλουμε το h2 και συγκεκριμένα όλα τα a (δηλαδή τα a href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Τραβάμε τα links και τα σώζουμε σε μια λίστα\n",
    "links = doc.select(\"h2 a\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### είναι διπλά και εμείς θέλουμε τα μονά "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = set(links)\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Μπορούμε να φιλτράρουμε σύμφωνα με λέξεις, αυτό έχει νόημα όταν ψάχνουμε κάποια συγκεκριμένη θεματολογία. Ας φιλτράρουμε π.χ. για economic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [link for link in links if 'economic' in link.text.lower()]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops στα Pandas!\n",
    "\n",
    "Για να κρατήσουμε μόνο το ωφέλιμο λινκ χωρίς την html χρησιμοποιούμε ένα loop μέσα στα [ ]. \n",
    "\n",
    "Αυτό ονομάζεται *list comprehension*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [link['href'] for link in links]\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βλέπουμε ότι οι τίτλοι εμφανίζονται μέσα στο λινκ. \n",
    "Ας τους τραβήξουμε με τον τρόπο που μάθαμε!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [link.text for link in links]\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Φτιάχνουμε το function για το scraping που θα το χρησιμοποιήσουμε για να πάρουμε την λίστα με τα λινκς των άρθρων!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Είσοδος: μια λίστα (base list) με τα links που έχουν τα URLs που θα συμπεριληφθούν στην λίστα για scraping\n",
    "# Έξοδος: λίστα με τα scraped links\n",
    "#Θα μπορούσα να είχα φτιάξει μια λίστα με λέξεις accepted_words, ώστε να τσέκαρα μόνο για συγκεκριμένα keywords μέσα στα links\n",
    "def get_scraped_links(base_list): #, accepted_words):\n",
    "    # Ανοίγουμε μια άδεια λίστα μέσα στην οποία στην συνέχεια θα μπουν τα scraped links\n",
    "    scraped_links = []\n",
    "    \n",
    "    # Ελέχγει ένα - ένα όλα τα URLs μέσα στην base list για να τραβήξειόλα τα links που βρίσκονται μέσα στα αρχικά links \n",
    "    # Προσθέτει τα νέα links στην λίστα με τα scraped links\n",
    "    for url in base_list:\n",
    "        parent = 'https://' + url.split('/')[2]\n",
    "        req = requests.get(url, headers)\n",
    "        print(req)\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        \n",
    "        # Για όλα τα links στα οποία αναφέρεται το link που βρίσκεται μέσα στο base_list:\n",
    "        for link in soup.select(\"h2 a\"):\n",
    "            \n",
    "            # Τράβα το url. Αν πρόκειται για εσωτερικό link που δεν ξεκινά με το domain, φτιάχνει το full URL ώστε να το κάνουμε scrape μετά\n",
    "            try:\n",
    "                l = link.get('href')\n",
    "                if l[0] == '/': \n",
    "                    l = parent + l\n",
    "                    print(l)\n",
    "                    scraped_links.append(l)\n",
    "            \n",
    "                # Τσεκάρει ότι μέσα στο URL υπάρχει έστω μια λέξη από τη λίστα accepted words\n",
    "                #if any(w in l for w in accepted_words):\n",
    "                    #scraped_links.append(l)\n",
    "            except:\n",
    "                print('no url')\n",
    "\n",
    "    # πετάει τα διπλά \n",
    "    scraped_links = list(set(scraped_links))\n",
    "    \n",
    "    # Τυπώνει το νούμερο των links που είναι scraped\n",
    "    print(len(scraped_links))\n",
    "    \n",
    "    return scraped_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Πάμε να φτιάξουμε μια λούπα για να πάρουμε πρώτα τα links των επιμέρους σελίδων του website. \n",
    "Παρατηρούμε ότι υπάρχουν 343 σελίδες, άρα..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "pages = []\n",
    "for i in range(0,342):\n",
    "    num = num + 1\n",
    "    #print(num)\n",
    "    page = 'https://iea.org.uk/blog/'\n",
    "    newpage = page + '?fwp_paged='+ str(num)\n",
    "    pages.append(newpage)\n",
    "    print(newpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#έλεγχος\n",
    "pages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = get_scraped_links(pages) #, accepted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Φτιάχνω ένα function για να πάρω το κάθε λινκ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "headers = {'user-agent': 'my-app/0.0.1'}\n",
    "PAUSE_TIME = 3.5\n",
    "\n",
    "def get_text(url):\n",
    "    r = requests.get(url, headers = headers)\n",
    "    time.sleep(PAUSE_TIME)\n",
    "    s = BeautifulSoup(r.content, 'html.parser')\n",
    "    link = s.find(\"h2 a\")\n",
    "    \n",
    "    return([s, link])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Κάνω scraping κάθε σελίδα για να μαζέψω όλα τα links των άρθρων"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_links = []\n",
    "titles = []\n",
    "counter = 0\n",
    "base_url = \"https://iea.org.uk/blog/?fwp_paged=\"\n",
    "for i in range(0,342):\n",
    "    url = base_url + str(i)\n",
    "    #print(url)\n",
    "    page = requests.get(url, headers = headers)\n",
    "    s = BeautifulSoup(page.content, 'html.parser')\n",
    "    #print(s)\n",
    "    links = s.select(\"h2 a\")\n",
    "    links = set(links)\n",
    "    \n",
    "    url = [link['href'] for link in links]\n",
    "    \n",
    "    #title = [link.text for link in links]\n",
    "    \n",
    "    \n",
    "    print(url)\n",
    "\n",
    "    for l in url:\n",
    "        link = l\n",
    "        print(l)\n",
    "        scraped_links.append(l)\n",
    "\n",
    "\n",
    "    \n",
    "    # Τυπώνει το νούμερο των links που είναι scraped\n",
    "print(len(scraped_links)) \n",
    "\n",
    "scraped_links = (set(scraped_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scraped_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αφού μάζεψα τα links των άρθρων τα σώζω για να μην τα χάσω και στην συνέχεια θα γράψω κώδικα που θα παίρνει τα στοιχεία του κάθε άρθρου ξεχωριστά!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(scraped_links, columns= ['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"iea.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = df['url'].tolist()\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Πάμε να κάνουμε scrape την κάθε σελίδα που περιέχει ένα άρθρο\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://iea.org.uk/the-low-hanging-fruit-on-the-brexit-tree-repeal-the-tobacco-products-directive/\", headers=headers)\n",
    "\n",
    "doc = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Τίτλος\n",
    "titles = doc.select(\"h2\")[0].text\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Συγγραφέας\n",
    "author = doc.select(\"div.ph-header-block div\")[1].text.strip()\n",
    "author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ημερομηνία\n",
    "date = doc.select(\"div.ph-header-block div\")[2].text.strip()\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Κείμενο\n",
    "article = doc.select(\"#modal-ready\")[0].text\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Πάμε να κάνουμε το ίδιο για όλα τα urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import time\n",
    "\n",
    "\n",
    "#def remove_non_ascii(text):\n",
    "#    return ''.join([i if ord(i) < 128 else ' ' for i in text])\n",
    "\n",
    "list=[]\n",
    "for i in urls[0:15]:  #βάζω συγκεκριμένο αριθμό για τεστ\n",
    "    dic = {}\n",
    "    #url = base_url + str(i)\n",
    "    print(i)\n",
    "    page = requests.get(i, headers = headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    \n",
    "    title = soup.select(\"h2\")\n",
    "\n",
    "    if (title):\n",
    "        dic['title'] = soup.select(\"title\")[0].text\n",
    "    else:\n",
    "        print(\"ARTICLE NO: \", link, \" HAS NO TITLE\")\n",
    "        dic['title'] = \"NO TITLE\"\n",
    "    \n",
    "        \n",
    "    date = soup.select(\"div.ph-header-block div\")\n",
    "    if (date):\n",
    "        dic['date'] = soup.select(\"div.ph-header-block div\")[2].text.strip()\n",
    "    else:\n",
    "        print(\"ARTICLE NO: \", link, \" HAS NO date\")\n",
    "        dic['date'] = \"NO date\"\n",
    "        \n",
    "        \n",
    "    author = soup.select(\"div.ph-header-block div\")\n",
    "    if (author):\n",
    "        dic['author'] = soup.select(\"div.ph-header-block div\")[1].text.strip()\n",
    "    else:\n",
    "        print(\"ARTICLE NO: \", link, \" HAS NO author\")\n",
    "        dic['author'] = \"NO author\"        \n",
    "        \n",
    "    \n",
    "    article =  soup.select(\"#modal-ready\")\n",
    "    if (article):\n",
    "        dic['body'] = soup.select(\"#modal-ready\")[0].text\n",
    "    else:\n",
    "        print(\"ARTICLE NO: \", link, \" HAS NO body\")\n",
    "        dic['body'] = \"NO body\"  \n",
    "    #article =  soup.select(\"#modal-ready\")[0].text\n",
    "    \n",
    "    #article_text = [a.text for a in article]\n",
    "    #dic['body'] = article\n",
    "    \n",
    "    dic['url'] = i\n",
    "\n",
    "    \n",
    "    list.append(dic)\n",
    "    #print(dic)\n",
    "    \n",
    "    time.sleep(0.5) #ορίζω πόση ώρα θα περιμένει μέχρι να ξαναχτυπήσει τον σέρβερ για να πάρει το επόμενο\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Κάνω την λίστα dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Πετάω τυχον διπλά"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['url'], keep='first', inplace = True)\n",
    "df.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Και σώζω το dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"iea_articles.csv\", index = False)\n",
    "df = pd.read_csv(\"iea_articles.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Κάνω τις ημερομηνίες τύπο datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime as dt\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Φτιάχνω ένα νέο column για τον χρόνο"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['date'].dt.strftime('%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Σορτάρω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['date'], inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Κρατάω μόνο όσα άρθρα έχουν κείμενο και ημερομηνία."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['body'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['date'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ξανασώζω το καθαρό df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"iea_articles.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
